# üõí E-commerce Sales Data Pipeline

**Data Engineering Mini-Project ‚Äì Artefact CI**

## üìå Contexte

Ce projet s‚Äôinscrit dans le cadre d‚Äôun challenge technique Data Engineer.  
L‚Äôobjectif est de concevoir et d√©ployer un **pipeline de donn√©es complet**, depuis l‚Äôanalyse exploratoire jusqu‚Äô√† l‚Äôorchestration avec **Apache Airflow**, en s‚Äôappuyant sur des technologies standards du Data Engineering.

Le jeu de donn√©es repr√©sente les ventes d‚Äôun site e-commerce et est stock√© dans un **bucket MinIO**. Les donn√©es sont ensuite **ing√©r√©es, normalis√©es et stock√©es dans PostgreSQL**, selon un mod√®le relationnel avanc√©.

---

## üéØ Objectifs du projet

- Analyser un jeu de donn√©es m√©tier r√©el
- Concevoir un mod√®le de donn√©es normalis√© (**3FN et DKNF**)
- Impl√©menter le mod√®le dans **PostgreSQL**
- D√©ployer l‚Äôinfrastructure avec **Docker & Docker Compose**
- D√©velopper un script Python d‚Äôingestion **idempotent**
- Orchestrer l‚Äôingestion avec **Apache Airflow**
- Structurer un projet de mani√®re professionnelle

---

## üß± Architecture globale

```text
CSV (Minio / S3)
      ‚Üì
Tables DKNF (PostgreSQL)
      ‚Üì
Vue analytique en √©toile
      ‚Üì
Exploitation BI / Analytics
```

---

## üõ†Ô∏è Technologies utilis√©es

- **Langage** : Python 3
- **Base de donn√©es** : PostgreSQL
- **Stockage objet** : MinIO (S3 compatible)
- **Orchestration** : Apache Airflow 3.x
- **Conteneurisation** : Docker & Docker Compose
- **SQL** : PostgreSQL compatible

---

## üìÅ Structure du projet

```text
‚îú‚îÄ‚îÄ üìÅconfig/
‚îú‚îÄ‚îÄ üìÅdags/
‚îÇ ‚îî‚îÄ‚îÄ üìÑfashion_sales_dag.py
‚îú‚îÄ‚îÄ üìÅingestion/
‚îÇ ‚îî‚îÄ‚îÄ üìÑmain.py
‚îú‚îÄ‚îÄ üìÅlogs/
‚îú‚îÄ‚îÄ üìÅminio-data/
‚îÇ ‚îî‚îÄ‚îÄ üìÑfashion_store_sales _ Data Eng.csv
‚îú‚îÄ‚îÄ üìÅplugins/
‚îú‚îÄ‚îÄ üìÅsql/
‚îÇ ‚îú‚îÄ‚îÄ üìÑ01_create_tables.sql
‚îÇ ‚îî‚îÄ‚îÄ üìÑ02_view_sales_star.sql
‚îú‚îÄ‚îÄ üìÑanalysis.ipynb
‚îú‚îÄ‚îÄ üìÑdocker-compose.yaml
‚îî‚îÄ‚îÄ üìÑREADME.md
```

---

## üîç Analyse exploratoire

Une analyse exploratoire a √©t√© r√©alis√©e afin de :

- comprendre la structure des donn√©es
- identifier les entit√©s m√©tier
- d√©tecter les redondances et anomalies
- pr√©parer la phase de mod√©lisation

üìÑ Livrable : `analysis.ipynb`

---

## üß© Mod√©lisation des donn√©es

- Normalisation jusqu‚Äô√† la **3·µâ forme normale (3FN)**
- Poursuite de la normalisation jusqu‚Äô√† la **DKNF**
- D√©finition claire des :
  - tables
  - cl√©s primaires
  - cl√©s √©trang√®res
- Cr√©ation d‚Äôune **vue en √©toile** pour faciliter l‚Äôanalyse

üìÑ Scripts SQL disponibles dans le dossier `sql/`

---

## üê≥ Lancer Docker

### Lancer PostgreSQL, MinIO, PGadmin & Airflow

```bash
docker compose up
```

- Les tables DKNF et la vue sont cr√©√©es automatiquement au d√©marrage
- Le fichier source est upload√© dans le bucket MinIO `folder-source`
- Initialisation automatique des variables et connexions requises par le DAG Airflow
- D√©clenchement automatique du DAG charg√© d‚Äôing√©rer les ventes du jour courant et peut √™tre ex√©cuter manuellement avec en param√®tre:

```json
{ "run_date": "20250616" }
```

Acc√®s UI :

- http://localhost:8080 -> acc√®s √† Airflow
- http://localhost:5050 -> acc√®s √† PGadmin
- http://localhost:9000 -> acc√®s √† Minio

Login :

- Airflow -> ( login : airflow, password : airflow )
- Minio -> ( login : minioadmin, password : minioadmin123 )
- PGadmin -> ( login : admin@local.com, password : adminpassword )
- postgres -> ( user : airflow, password: airflow, DB: artefect_db, host: postgres or localhost:5432 )

## üêçScript Python d‚Äôingestion

- Prend une date en param√®tre : YYYYMMDD
- Lit le fichier source depuis MinIO
- Filtre les ventes correspondant √† la date
- Alimente les tables PostgreSQL normalis√©es
- Idempotence garantie
- Gestion des erreurs (date, connexion, insertion)
- Logging int√©gr√©

**NB:** _Avant de lancer le script cr√©er un fichier `.env` √† la racine du projet contenant les lignes suivantes:_

```bash
AIRFLOW_UID=50000

# Minio
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_SECURE=false
MINIO_BUCKET=folder-source
MINIO_OBJECT_KEY=fashion_store_sales _ Data Eng.csv

# Postgres
PG_HOST=localhost
PG_PORT=5432
PG_DB=artefect_db
PG_USER=airflow
PG_PASSWORD=airflow
```

**Recommander:** cr√©er un environnement virtuelle python avant d'installer les dependants pour le lancement du script :

```bash
pip install -r requirements.txt
```

### Exemple d‚Äôex√©cution

```bash
python ingestion/main.py 20250616
```

**NB:** Assurer vous d'avoir tous les containers docker qui tourne bien avant le lancement du script

## ‚úÖ Points cl√©s techniques

- Pipeline reproductible et idempotent
- S√©paration claire des responsabilit√©s
- SQL robuste avec contraintes d‚Äôint√©grit√©
- Dockerisation compl√®te
- Orchestration fiable et maintenable

## üë§ Auteur

Projet r√©alis√© par :

**Kouam√© Antonio Parfait**

Data Engineer
